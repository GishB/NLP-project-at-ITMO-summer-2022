# -*- coding: utf-8 -*-
"""Homework_NLP_MachineLearning_Spring_Samofalov

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KA3I2xw5m-0LhxQE-35v5lKINwbqxbH0

# Пользовательские функции на замену sklearn для оценки качества модели
"""

def edited_list_function(take):
  new_list = []
  for i in take:
    temp_list = []
    for g in i:
      if g > 0.5:
        temp_list.append(1)
      else:
        temp_list.append(0)
    new_list.append(temp_list)
  return np.array(new_list)

def accuracy_custom(y_true,y_score):
  if np.min(y_score) < 0:
    y_score = edited_list_function(y_score)
  true_labels_count = []
  for g in range(len(y_true[0])):
    count = 0
    for i in range(len(y_true)):
      if y_true[i][g] == y_score[i][g]:
        count += 1
    true_labels_count.append(count)
  number_test = len(y_score)

  list_accuracy_custom = []
  for i in range(len(true_labels_count)):
    temp = int((true_labels_count[i]/number_test)*100)
    list_accuracy_custom.append(temp)

  return np.array(list_accuracy_custom)


def accuracy_average(y_true,y_score):
  temp = accuracy_custom(y_true,y_score)
  return np.mean(temp)


def true_positive(y_true,y_score):
  if np.min(y_score) < 0:
    y_score = edited_list_function(y_score)

  true_positive_list = []
  for g in range(len(y_true[0])):
    count = 0
    for i in range(len(y_true)):
      if y_true[i][g] == 1:
        if y_score[i][g] == 1:
          count += 1
    true_positive_list.append(count)
  return true_positive_list

def false_positive(y_true,y_score):
  if np.min(y_score) < 0:
    y_score = edited_list_function(y_score)
  false_positive_list = []
  for g in range(len(y_true[0])):
    count = 0
    for i in range(len(y_true)):
      if y_true[i][g] != 1:
        if y_score[i][g] == 1:
          count += 1
    false_positive_list.append(count)
  return false_positive_list

def true_negative(y_true,y_score):
  if np.min(y_score) < 0:
    y_score = edited_list_function(y_score)
  true_negative_list = []
  for g in range(len(y_true[0])):
    count = 0
    for i in range(len(y_true)):
      if y_true[i][g] == 0:
        if y_score[i][g] == 0:
          count += 1
    true_negative_list.append(count)
  return true_negative_list

def false_negative(y_true,y_score):
  if np.min(y_score) < 0:
    y_score = edited_list_function(y_score)

  false_negative_list = []
  for g in range(len(y_true[0])):
    count = 0
    for i in range(len(y_true)):
      if y_true[i][g] != 0:
        if y_score[i][g] == 0:
          count += 1
    false_negative_list.append(count)
  return false_negative_list

def recall_custom(y_true,y_score):

  if np.min(y_score) < 0:
    y_score = edited_list_function(y_score)

  tp = true_positive(y_true,y_score)
  fn = false_negative(y_true,y_score)
  recall_list = []
  for i in range(len(tp)):
    try:
      recall = tp[i]/(tp[i]+fn[i])
      recall_list.append(recall)
    except:
      recall_list.append('NaN')
      print('Деление на 0 - ошибка! Recall')
  return recall_list

def precision_custom(y_true,y_score):

  if np.min(y_score) < 0:
    y_score = edited_list_function(y_score)

  tp = true_positive(y_true,y_score)
  fp = false_negative(y_true,y_score)
  precision_list = []
  for i in range(len(tp)):
    try:
      recall = tp[i]/(tp[i]+fp[i])
      precision_list.append(recall)
    except:
      precision_list.append('NaN')
      print('Деление на 0 - ошибка! Precision')
  return precision_list


def f1_custom(y_true,y_score):

  if np.min(y_score) < 0:
    y_score = edited_list_function(y_score)

  recall = recall_custom(y_true,y_score)
  precision = precision_custom(y_true,y_score)
  f1_list = []
  for i in range(len(recall)):
    try:
      f1 = 2*precision[i]*recall[i]/(precision[i] + recall[i])
      f1_list.append(f1)
    except:
      f1_list.append('NaN')
      print('Деление на 0 - ошибка! F1')
  return np.array(f1_list)

"""# Загрузка необходимых библиотек"""

!python -m pip install 'fsspec>=0.3.3'

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

import re

import nltk
nltk.download('stopwords') #для подготовки текста (первоначальной)
nltk.download('punkt')

from nltk.tokenize import word_tokenize
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords
from nltk.tokenize import regexp_tokenize
from nltk.tokenize import PunktSentenceTokenizer
from sklearn.feature_extraction.text import CountVectorizer

import dask.bag as db
import json


!pip install openpyxl
import openpyxl


from keras.preprocessing.text import Tokenizer
from keras_preprocessing.sequence import pad_sequences

from sklearn.model_selection import train_test_split

from gensim.models import Word2Vec
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPool1D, Dropout, Dense, GlobalMaxPool1D, Embedding, Activation

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
# from keras.preprocessing.sequence import pad_sequences
from sklearn import preprocessing


# from sklearn import preprocessing
from gensim.models import Word2Vec

"""# Загрузка данных из папки"""

ai_category_list=['stat.ML','cs.LG','cs.AI']
records=db.read_text(r"/content/drive/MyDrive/Archiv/arxiv-metadata-oai-snapshot.json").map(lambda x:json.loads(x))
ai_docs = (records.filter(lambda x:any(ele in x['categories'] for ele in ai_category_list)==True))
get_metadata = lambda x: {'id': x['id'],
                  'title': x['title'],
                  'category':x['categories'],
                  'abstract':x['abstract'],
                 'version':x['versions'][-1]['created'],
                         'doi':x["doi"],
                         'authors_parsed':x['authors_parsed']}

data=ai_docs.map(get_metadata).to_dataframe().compute()

data.to_excel("AI_ML_ArXiv_Papers.xlsx",index=False,encoding="utf-8")

data.head(4)

"""# Предобработка текста"""

def clean_review_simple(review):
    review = re.sub("[^A-Za-z]", " ", review)
    review = review.lower()
    review = word_tokenize(review)
    stemmer = PorterStemmer()
    review = " ".join(review)
    return review  


def clean_text(data):
    data = list(data)
    text_cleaned = []
    for i in range(len(data)):
        cleaned_text = clean_review_simple(data[i])
        text_cleaned.append(cleaned_text)
    return text_cleaned

titles_cleaned, abstract_cleaned = clean_text(data['title']), clean_text(data['abstract'])

"""# Определяем уникальные значения для вектора  по всем категориям """

category_list = [] #Выбираем первый лейб статьи как основной
for i in range(len(data)):
    split_category = data['category'][i].split()
  #  number_len = len(split_category)
  #  category_list.append(split_category[np.random.randint(0, number_len)])
    # temp_list = []
    # for g in range(len(split_category)):
    #     g_test = split_category[g]
    #     temp_list.append(g_test)
    category_list.append(split_category[0])
    


df_category = pd.DataFrame(category_list)        
category_unique_test = df_category[0].unique() #Создаем список уникальный лейблов в датасете

category_unique_test

#Альтернатива в виде векторов для labels
list_of_vectors = []
len_vector = len(category_unique_test)


for i in range(len(data)):
  temp_vector = np.zeros(len_vector)
  temp_categorical_labels = data['category'][i].split()
  for g in range(len(temp_categorical_labels)):
    temp_categorical_label = temp_categorical_labels[g]
    for k in range(len_vector):
      if temp_categorical_label == category_unique_test[k]:
        temp_vector[k] = 1
      else:
        pass
  list_of_vectors.append(temp_vector)

"""# Создаем общий датафрейм"""

# vector_labels_df = pd.DataFrame(category_list, columns= ['Labels']) 
abstract_df = pd.DataFrame(abstract_cleaned, columns=['Abstract Cleaned'])
titles_df = pd.DataFrame(titles_cleaned, columns=['Titles Cleaned'])

general_df = pd.concat([abstract_df, titles_df], axis=1)

general_df['Summary'] = general_df['Titles Cleaned'] + ' ' + general_df['Abstract Cleaned']

general_df = general_df.drop(axis = 1, columns=['Titles Cleaned', 'Abstract Cleaned'])

general_df#вектор с значениями хранится по объекту list_of_category

"""# Токенизация"""

token = Tokenizer(89740)
token.fit_on_texts(general_df['Summary'])
token_text = token.texts_to_sequences(general_df['Summary'])
token_text = pad_sequences(token_text)

len(token_text[0])

#from keras.utils import to_categorical #необходимо для вектора с предсказанием класса
# from tensorflow.keras.utils import to_categorical

# # la = preprocessing.LabelEncoder()
# # y = la.fit_transform(data['category'])
# # y = to_categorical(y)
# # print(y[:5])

"""# Загружаем word2vec model"""

t_d = []
for i in general_df['Summary']:
    t_d.append(i.split())
print(t_d[:2]) #подготовка данных для модели word2vec

w2v_model = Word2Vec(t_d, workers=32, min_count=1, window=4)
print(w2v_model)

"""# Разбивка dataframe на обучающую и тестовую выборку (range(0,4) - это какие лейблы мы предсказываем из списка category_unique_test)

"""

vect_list = []
for i in data['category']:
  data_temp = i.split()
  vect_temp = []
  for g in range(0,10):
    if category_unique_test[g] in data_temp:
      vect_temp.append(1)
    else:
      vect_temp.append(0)
  vect_list.append(vect_temp)

vect_list = np.array(vect_list)

X_train, X_test, y_train, y_test = train_test_split(np.array(token_text), vect_list, test_size=0.7)

"""# Загружаем модель №1: word2vec + MLP"""

def gensim_to_keras_embedding(model, train_embeddings=True):

    keyed_vectors = model.wv  # structure holding the result of training
    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    
    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?

    layer = Embedding(
        input_dim=weights.shape[0],
        output_dim=weights.shape[1],
        weights=[weights],
        trainable=train_embeddings,
    )
    return layer

# import BatchNormalization
from tensorflow.keras.layers import BatchNormalization

#загрузим тестовую модель
keras_model_vect = Sequential()

keras_model_vect.add(w2v_model.wv.get_keras_embedding(train_embeddings=True))
#keras_model.add(gensim_to_keras_embedding(w2v_model)) #в некоторых случаях надо использовать custom layer - https://github.com/RaRe-Technologies/gensim/wiki/Using-Gensim-Embeddings-with-Keras-and-Tensorflow

keras_model_vect.add(GlobalMaxPool1D())

keras_model_vect.add(Activation('relu'))
keras_model_vect.add(Dropout(0.2))
keras_model_vect.add(Dense(1024))
keras_model_vect.add(BatchNormalization())

keras_model_vect.add(Activation('relu'))
keras_model_vect.add(Dropout(0.2))
keras_model_vect.add(Dense(1024/2))
keras_model_vect.add(BatchNormalization())

keras_model_vect.add(Activation('relu'))
keras_model_vect.add(Dropout(0.2))
keras_model_vect.add(Dense(1024/4))
keras_model_vect.add(BatchNormalization())

keras_model_vect.add(Activation('relu'))
keras_model_vect.add(Dropout(0.2))
keras_model_vect.add(Dense(1024/8))
keras_model_vect.add(BatchNormalization())

keras_model_vect.add(Activation('softmax'))
keras_model_vect.add(Dropout(0.1))
keras_model_vect.add(Dense(len(y_test[0])))

keras_model_vect.compile(loss='BinaryCrossentropy', metrics=['acc'], optimizer='Adam')
keras_model_vect.summary()

"""# Обучение модели №1"""

keras_model_vect.fit(np.array(X_train), np.array(y_train), batch_size=256, epochs=10, validation_data=(np.array(X_test), np.array(y_test)))

"""# Проверяем модель №1"""

predicted_1 = np.array(edited_list_function(keras_model_vect.predict(X_test)))

print(f'{f1_custom(y_test,predicted_1)} F1 score по labels')

print(f'{true_positive(y_test,predicted_1)} TP по labels')
print(f'{true_negative(y_test,predicted_1)} TN по labels')
print(f'{false_positive(y_test,predicted_1)} FP по labels')
print(f'{false_negative(y_test,predicted_1)} FN по labels')

print(f'{precision_custom(y_test,predicted_1)} precision по labels')
print(f'{recall_custom(y_test,predicted_1)} recall по labels')
print(f'{accuracy_custom(y_test,predicted_1)} accuracy по labels')
print(f'{accuracy_average(y_test,predicted_1)} average accuracy')

"""# Загружаем модель №2: MLP (подготовка данных аналогична модели №1 за исключением слоя word2vec в модели)"""

model_MLP = Sequential()

model_MLP.add(Dense(1024, input_dim=len(X_test[0])))

model_MLP.add(Activation('relu'))
model_MLP.add(Dropout(0.2))
model_MLP.add(Dense(1024))
model_MLP.add(BatchNormalization())

model_MLP.add(Activation('relu'))
model_MLP.add(Dropout(0.2))
model_MLP.add(Dense(1024/2))
model_MLP.add(BatchNormalization())

model_MLP.add(Activation('relu'))
model_MLP.add(Dropout(0.2))
model_MLP.add(Dense(1024/4))
model_MLP.add(BatchNormalization())

model_MLP.add(Activation('relu'))
model_MLP.add(Dropout(0.2))
model_MLP.add(Dense(1024/8))
model_MLP.add(BatchNormalization())

model_MLP.add(Activation('softmax'))
model_MLP.add(Dropout(0.1))
model_MLP.add(Dense(len(y_test[0])))

model_MLP.compile(loss='BinaryCrossentropy', metrics=['acc'], optimizer='adam')
model_MLP.summary()

"""# Обучение модели №2"""

# train the model
model_MLP.fit(np.array(X_train), np.array(y_train), batch_size=256, epochs=11, validation_data=(np.array(X_test), np.array(y_test)))

"""# Проверяем модель №2"""

predicted_2 = np.array(edited_list_function(model_MLP.predict(X_test)))

print(f'{f1_custom(y_test,predicted_2)} F1 score по labels')

print(f'{true_positive(y_test,predicted_2)} TP по labels')
print(f'{true_negative(y_test,predicted_2)} TN по labels')
print(f'{false_positive(y_test,predicted_2)} FP по labels')
print(f'{false_negative(y_test,predicted_2)} FN по labels')

print(f'{precision_custom(y_test,predicted_2)} precision по labels')
print(f'{recall_custom(y_test,predicted_2)} recall по labels')
print(f'{accuracy_custom(y_test,predicted_2)} accuracy по labels')
print(f'{accuracy_average(y_test,predicted_2)} average accuracy')

"""# Загружаем модель №3: LSTM + MLP (подготовка данных аналогична модели №1, за исключением слоя word2vec - используем LSTM)"""

#дополнительная подготовка данных для LSTM для 3-х мерного пространства
X_train_LSTM = np.expand_dims(np.array(X_train), 1)
X_test_LSTM = np.expand_dims(np.array(X_test), 1)

model_3 = Sequential()

model_3.add(layers.LSTM(1024, input_dim=len(X_train[0])))

model_3.add(Activation('relu'))
model_3.add(Dropout(0.2))
model_3.add(Dense(1024))
model_3.add(BatchNormalization())

model_3.add(Activation('relu'))
model_3.add(Dropout(0.2))
model_3.add(Dense(1024/2))
model_3.add(BatchNormalization())

model_3.add(Activation('relu'))
model_3.add(Dropout(0.2))
model_3.add(Dense(1024/4))
model_3.add(BatchNormalization())

model_3.add(Activation('relu'))
model_3.add(Dropout(0.2))
model_3.add(Dense(1024/8))
model_3.add(BatchNormalization())

model_3.add(Activation('softmax'))
model_3.add(Dropout(0.1))
model_3.add(Dense(len(y_test[0])))

model_3.compile(loss='BinaryCrossentropy', metrics=['acc'], optimizer='adam')
model_3.summary()

"""# Обучаем модель №3"""

# train the model
model_3.fit(X_train_LSTM, y_train, batch_size=256, epochs=11, validation_data=(X_test_LSTM, np.array(y_test)))

"""# Проверяем модель №3"""

predicted_3 = np.array(edited_list_function(model_3.predict(X_test_LSTM)))

print(f'{f1_custom(y_test,predicted_3)} F1 score по labels')

print(f'{true_positive(y_test,predicted_3)} TP по labels')
print(f'{true_negative(y_test,predicted_3)} TN по labels')
print(f'{false_positive(y_test,predicted_3)} FP по labels')
print(f'{false_negative(y_test,predicted_3)} FN по labels')

print(f'{precision_custom(y_test,predicted_3)} precision по labels')
print(f'{recall_custom(y_test,predicted_3)} recall по labels')
print(f'{accuracy_custom(y_test,predicted_3)} accuracy по labels')
print(f'{accuracy_average(y_test,predicted_3)} average accuracy')

y_test[0:10]

predicted_3[0:10]

"""#Вывод по задаче NLP "Предсказание класса исходя из его 

#Как происходила подготовка данных?

**По поводу Summary**

Данные по абстрактам и титулам прошли фильтрацию на стопсимволы, знаки, большие\малые буквы. 

Далее, текст абстракта и титула был объединен в один текст по каждой строчке столбца

Перед обучением данные токенизировались по отдельным словам и кодировались по номерам исходя из словаря

**По поводу Labels**

Циклом for по всему dataframe с классами статей для создания вектора произвольного размера (в нашем случае 10) из уникальный значений классов встречаемых в статьях (всего классов 119)



#Использованы три модели машинного обучения для задачи NLP


1.   word2vec + FCN
2.   MLP
3.   LSTM + FCN



#Основная проблема плохой классификации
  
1.   Разметка labels (стоит агрегировать данные по общему признаку - 119 классов это много)
2.   Колличество данных (из-за большого колличества уникальных классов категории в датафрейме не сбалансированны, как реузльтат модели плохо обучаются)


#Возможные пути решения проблемы

1.   В первую очередь, необходимо изменить подход к разметке данных - уменьшить колличество признаков в категориях до минимального кол-ва (скажем 10) с целью сбалансировать классы.

2. Необходимо, изучить влияние гиперпараментров при создании словаря word2vec

3. Необходимо, изучить влияние токенизации (можно токены делать по предложениям или вообще не удалять символы + стопслова и так далее. Более того, можно было бы текст рассматривать более сложной структурой [токены по словам,токены по предпложениям])


#Какие попытки были примененны для решения проблемы

1.   Задание была рассмотренно как задача многоклассовой классификации типа [0,0,0,1,0,0,1,1,0,0,0..,0] (увелечение кол-ва признаков вектора приводит к плохим результатм обучения модели)

2.   Искуственное уменьшение размерности за счет произвольного изменения длины вектора с классами. (увелечение точности модели и "способности" обучаться)

3.   Изменение структуры нейронных сетей. 
 
  1. повышение стабильности обучения при использовании слоя BatchNormalization.
  2. повышение точности за счет небольшого увелечения кол-ва и размера FCN слоев. 
  3. использование ReLU активации в скрытых слоях для борьбы с недообученностью.





"""

